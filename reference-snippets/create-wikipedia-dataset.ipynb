{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.parse\n",
    "import requests\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve Wikipedia articles matching a topic\n",
    "\n",
    "There doesn't seem to be a very easy way to get the full articles that are recursively related to a category.\n",
    "There is a tool called PetScan https://petscan.wmflabs.org/ that can find all related articles but it only returns ids, title and not the content.\n",
    "\n",
    "There is an API to get the full page but we would likely need to call it too many times.\n",
    "\n",
    "There is a special export UI that lets you list articles but it doesn't seem well suited to this usecase.\n",
    "\n",
    "What we will do is filter a datascience Wikipedia dump for the matching articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Provide the exact wikipedia category name \n",
    "category = \"Military equipment\"\n",
    "# The depth controls how deep the crawl goes\n",
    "depth = 3\n",
    "\n",
    "# We need to plus encode the category for the url\n",
    "category = urllib.parse.quote_plus(category)\n",
    "url =f'https://petscan.wmflabs.org/?depth={depth}&format=json&regexp_filter=&links_to_no=&cb_labels_any_l=1&search_query=&sparql=&ores_type=any&output_compatability=catscan&langs_labels_no=&max_sitelink_count=&templates_no=&search_max_results=500&wpiu=any&ores_prob_to=&referrer_name=&project=wikipedia&templates_any=&ns%5B0%5D=1&categories={category}&links_to_all=&ores_prediction=any&labels_no=&wikidata_item=no&interface_language=en&namespace_conversion=keep&manual_list_wiki=&sortby=none&show_soft_redirects=both&search_wiki=&wikidata_source_sites=&edits%5Bflagged%5D=both&labels_yes=&after=&cb_labels_yes_l=1&min_redlink_count=1&larger=&edits%5Banons%5D=both&common_wiki_other=&cb_labels_no_l=1&language=en&page_image=any&doit='\n",
    "response =  requests.get(url,headers={'Accept': 'application/json'})\n",
    "data = response.json()\n",
    "\n",
    "articles=data['*'][0]['a']['*']\n",
    "\n",
    "print(f'Found {len(articles)} articles from Pet Scan  for Category:{category} and depth:{depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Found {len(articles)} articles from Pet Scan  for Category:{category} and depth:{depth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just need the list of IDs so we can filter the main dataset\n",
    "articles=data['*'][0]['a']['*']\n",
    "ids=[]\n",
    "for article in articles:\n",
    "    ids.append(str(article['id']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the whole wikipedia dataset from the Hugging Face cleaned Dataset\n",
    "# This will take a while to download the ~20GB but it is then locally cached for this user\n",
    "wiki=load_dataset(\"wikipedia\", \"20220301.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This is obviously not very efficient and there maybe a better way to do this \n",
    "# In my testing this was taking about 1h\n",
    "\n",
    "local_filename = f'./wiki_articles_category_{category}_{depth}'\n",
    "\n",
    "wiki_articles = wiki['train']\n",
    "try:\n",
    "    wiki_match_category.load_from_disk(local_filename)\n",
    "    print('Sucessfully loaded data from disk')\n",
    "except:\n",
    "    print('Filtering the dataset')\n",
    "    wiki_match_category = wiki_articles.filter(lambda x: x['id'] in ids)\n",
    "    wiki_match_category.save_to_disk(local_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_docs = len(wiki_match_category)\n",
    "print(f'Extracted and saved {number_of_docs} articles')\n",
    "print(f'This maybe slightly different from the original {len(articles)} due to a missmatch in the current version and the historic, cleaned data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
