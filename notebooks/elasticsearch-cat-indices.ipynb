{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elasticsearch cat_indices Jupyter Notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This Jupyter Notebook demonstrates how to fetch data from an Elasticsearch cluster and process it to generate a CSV file. The data includes Elasticsearch indices information, which is processed to provide insights into the data types and sizes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from getpass import getpass\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_es_data(es_url: str, username: str, password: str):\n",
    "    \"\"\"\n",
    "    Fetch data from Elasticsearch using a GET request.\n",
    "\n",
    "    Args:\n",
    "    es_url (str): URL of the Elasticsearch instance.\n",
    "    username (str): Username for Elasticsearch authentication.\n",
    "    password (str): Password for Elasticsearch authentication.\n",
    "\n",
    "    Returns:\n",
    "    requests.Response: The response object from the GET request.\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform a GET request to the Elasticsearch _cat/indices endpoint.\n",
    "    # The authentication credentials are passed for accessing the server.\n",
    "    return requests.get(f\"{es_url}/_cat/indices?v\", auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_size(size_str: str):\n",
    "    \"\"\"\n",
    "    Convert a size string to bytes. The input string is expected to be binary (e.g., '10KiB', not '10kB')\n",
    "\n",
    "    Args:\n",
    "    size_str (str): The string representing the size. Examples: '10kb', '2mb'.\n",
    "\n",
    "    Returns:\n",
    "    float: The size in bytes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the conversion units from various size units to bytes.\n",
    "    size_units = {\"kb\": 1024, \"mb\": 1024**2, \"gb\": 1024**3, \"tb\": 1024**4, \"b\": 1}\n",
    "\n",
    "    # Normalize the size string for consistent parsing.\n",
    "    size_str = size_str.lower().replace(\",\", \".\")\n",
    "\n",
    "    # Extract the number and unit from the size string and calculate the size in bytes.\n",
    "    for unit in size_units:\n",
    "        if unit in size_str:\n",
    "            return float(re.findall(r\"\\d+\\.?\\d*\", size_str)[0]) * size_units[unit]\n",
    "\n",
    "    # otherwise, return 0.0 if no match\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_es_indices(es_url: str, output_csv: str):\n",
    "    \"\"\"\n",
    "    Fetch Elasticsearch indices data and write a summary to a CSV file.\n",
    "\n",
    "    Args:\n",
    "    es_url (str): URL of the Elasticsearch instance.\n",
    "    output_csv (str): File path for the output CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt the user for Elasticsearch credentials.\n",
    "    username = input(\"Enter Elasticsearch username: \")\n",
    "    password = getpass(\"Enter Elasticsearch password: \")\n",
    "\n",
    "    # Fetch data from the Elasticsearch instance.\n",
    "    response = fetch_es_data(es_url, username, password)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error fetching data from Elasticsearch: {response.text}\")\n",
    "        return\n",
    "\n",
    "    # Split the response text into individual lines for further processing.\n",
    "    lines = response.text.strip().split(\"\\n\")\n",
    "\n",
    "    data_list = []\n",
    "    # Extract headers (column names) from the first line of the response.\n",
    "    headers_list = lines[0].split()\n",
    "\n",
    "    # Convert each line of data into a dictionary and append to the data list.\n",
    "    for line in lines[1:]:\n",
    "        line = line.split()\n",
    "        data_list.append(dict(zip(headers_list, line[0:])))\n",
    "\n",
    "    # Convert size values in the data list from strings to bytes.\n",
    "    for data in data_list:\n",
    "        data[\"pri.store.size\"] = parse_size(data[\"pri.store.size\"])\n",
    "        data[\"store.size\"] = parse_size(data[\"store.size\"])\n",
    "        data[\"dataset.size\"] = parse_size(data[\"dataset.size\"])\n",
    "\n",
    "    # Write the processed data to a CSV file.\n",
    "    with open(output_csv, \"w\", newline=\"\") as file:\n",
    "        # Create a csv.DictWriter object to write dictionaries to a CSV.\n",
    "        writer = csv.DictWriter(file, fieldnames=data_list[0].keys())\n",
    "\n",
    "        # Write column headers to the CSV file.\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Write each row of data to the CSV file.\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Elasticsearch URL and output CSV file path\n",
    "es_url = getpass(\"Enter Elasticsearch URL: \")  # Replace with your Elasticsearch URL\n",
    "output_csv = \"output.csv\"\n",
    "\n",
    "# Call the function to process data\n",
    "process_es_indices(es_url, output_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we demonstrated how to interact with an Elasticsearch cluster using Python to fetch and process data. This script can be modified to suit different Elasticsearch configurations or data processing requirements.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
